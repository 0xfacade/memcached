Storage system notes
--------------------

extstore.h defines the API.

extstore_write() is a synchronous call which memcpy's the input buffer into a
write buffer for an active page. A failure is not usually a hard failure, but
indicates caller can try again another time. IE: it might be busy freeing
pages or assigning new ones.

as of this writing the write() implementation doesn't have an internal loop,
so it can give spurious failures (good for testing integration)

extstore_read() is an asynchronous call which takes a stack of IO objects and
adds it to the end of a queue. It then signals the IO thread to run. Once an
IO stack is submitted the caller must not touch the submitted objects anymore
(they are relinked internally).

extstore_delete() is a synchronous call which informs the storage engine an
item has been removed from that page. It's important to call this as items are
actively deleted or passively reaped due to TTL expiration. This allows the
engine to intelligently reclaim pages.

The IO threads execute each object in turn (or in bulk of running in the
future libaio mode).

Callbacks are issued from the IO threads. It's thus important to keep
processing to a minimum. Callbacks may be issued out of order, and it is the
caller's responsibility to know when its stack has been fully processed so it
may reclaim the memory.

With DIRECT_IO support, buffers submitted for read/write will need to be
aligned with posix_memalign() or similar.

Buckets
-------

I had described this as "TTL aware", however it makes more sense for the
caller to decide how to sort data into active pages. During extstore_init(), a
number of active buckets is specified. Pages are handled overall as a global
pool, but writes can be redirected to specific active pages.

This allows a lot of flexibility, ie:

1) The original idea of "high TTL" and "low TTL" being two buckets. TTL <
86400 goes into bucket 0, rest into bucket 1. Co-locating low TTL items means
those pages can reach zero objects and free up more easily.

2) Extend this: "low TTL" is one bucket, and then one bucket per slab class.
If TTL's are low, mixed sized objects can go together as they are likely to
expire before cycling out of flash (depending on workload, of course).
For higher TTL items, pages are stored on chunk barriers. This means less
space is wasted as items should fit nearly exactly into write buffers and
pages. It also means you can blindly read items back if the system wants to
free a page and we can indicate to the caller somehow which pages are up for
probation. ie; issue a read against page 3 version 1 for byte range 0->1MB,
then chunk and look up objects. Then read next 1MB chunk/etc. If there's
anything we want to keep, pull it back into RAM before pages is freed.

Pages are assigned into buckets on demand, so if you make 30 but use 1 there
will only be a single active page with write buffers.

Memcached integration
---------------------

With the POC: items.c's lru_maintainer_thread calls writes to storage if all
memory has been allocated out to slab classes, and there is less than an
amount of memory free. Original objects are swapped with items marked with
ITEM_HDR flag. an ITEM_HDR contains copies of the original key and most of the
header data. The ITEM_data() section of an ITEM_HDR object contains (item_hdr
*), which describes enough information to retrieve the original object from
storage.

To get best performance is important that reads can be deeply pipelined.
As much processing as possible is done ahead of time, IO's are submitted, and
once IO's are done processing a minimal amount of code is executed before
transmit() is possible. This should amortize the amount of latency incurred by
hopping threads and waiting on IO.

Compaction etc
--------------

Storage pages contain version values. During a fetch the version value from the 
ITEM_HDR object needs to be
compared to the current existing page, and a read will fail if the page has
been wiped since the header object was created.

Pages will be reaped by an estimate of how few real objects it still contains.

[TBD]

TODO
----

Sharing my broad TODO items into here. While doing the work they get split up
more into local notes. Adding this so others can follow along:

x extstore_delete() function to decrement object count for a page
x figure out page refcounting
x page version validation during read.
  x return a miss into the callback from IO thread? Probably, because anything
    else would be a race condition so better to have one code path.
- pages readying to be reaped mark as "deprecated" (closed == true).
  - add function a crawler could use to tell if an item's page is going to be reaped
    "soon" (thus chuck back into memory)
  - *maybe* a callback that hands sections or the entirety of a page about to
    be nuked.
  - want to keep this simple; objects are not individually tracked, so a page
    reclaim simple wipes the storage out from under headers.
    - however, if very compact representation of objects -> offset/len/TTL
      could be kept in memory (possibly just for "low TTL" pages), one could
      callback for each tracked object that it thinks are still valid. Since
      the written object should be a mirror (or close to) the original memory
      object, a normal lookup could be done and the data swapped back into
      cache or written back to a new page.
    - at 4b per off/len/ttl against 1k objects in a 64 meg page, that's .8m of
      memory per 64m of flash? not great. If DIRECT_IO is in use could cut to
      2b for off/len easy (multiples of sectors)
    - ^^^ being able to recover valid items from an aged out page is a
      second-pass goal.
x sort write buffers properly (collapse IO code; enforce 2 buffers per active
  page)
  - did this without enforcing a buffer limit. didn't collapse IO code yet.
o TTL histogram aging for maintenance thread
  - replaced with arbitrary buckets
- DIRECT_IO support
- libaio support (requires DIRECT_IO)
- TRIM support
- some stats counters that can be polled
  - page turnover, writes, reads, trims, etc.
- code cleanup (funtion over form until I have bugs out)
  - flatten write() properly.
  - flatten read thread properly.
  - pull all of the inlined linked list code
  - naming consistency
  - clear FIXME/TODO's related to error handling
- JBOD support (also not first pass)
  - 1-2 active pages per device. potentially dedicated IO threads per device.
    with a RAID setup you risk any individual disk doing a GC pause stalling
    all writes. with many disks these stalls can become constant.

on memcached end:
- shunt to flash if tail of COLD has age > N or if free memory is below X
  - and remaining TTL is > Y
- re-raise from flash into WARM if hit more than once in N seconds (60?), with
  a percentage chance (ex: objects getting hit multiple times within 60
  seconds get a 20% chance of pulling back into WARM, deleting from flash)
  - better if the percentage or age could dynamically raise by the amount of
    idle time the IO threads have.
  - not a first pass task
  - requires item CAS be enabled (compared CAS for ITEM_HDR object with what
    was on flash)
- complete code paths for miss
- fix append/prepend/incr/decr/etc
- proper counters/documentation (flash hit/miss rate)
- proper startup options
- --configure gating for extstore being compiled (for now, at least)
- binprot support
- crc32 checking (hardware/non hardware). embed in object header, or overwrite
  some non-essential bytes of original object before sending to flash
- DIRECT_IO support; mostly memalign pages, but also making chunks grow
  aligned to sector sizes once they are >= a single sector.
